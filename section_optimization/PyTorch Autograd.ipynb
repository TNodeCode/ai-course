{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e57671b-a114-4df1-b198-dc0b80481030",
   "metadata": {},
   "source": [
    "# PyTorch's Autograd Functionality\n",
    "\n",
    "If we have large neural networks, it would be complicated to implement the gradient descent algorithm by hand. In this notebook we will show you that PyTorch can compute the derivatives of any function you run a vector through.\n",
    "\n",
    "Let's start with a simple example. We have a one-dimensional vector, also called a scalar, and a function $f_1(x) = x^2$. The derivative of $f_1$ with respect to $x$ is: $\\frac{\\partial f_1}{\\partial x} = 2x$.\n",
    "\n",
    "So for our value $x = 5$ the gradient produced by $x$ when it is run through $f_1$ would be $2 \\cdot 5 = 10$.\n",
    "\n",
    "Will now show you how you can compute the gradient with PyTorch automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a02067-58a4-44bf-9ed2-7e6217d4e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the vector as a PyTorch tensor. We also need to set the attribute 'requires_grad' to True\n",
    "# so that PyTorch computes keeps track of the operations applied on this tensor.\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "\n",
    "# Define the function f1(x) = x^2\n",
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "# Apply the function to the tensor\n",
    "y = f1(x)\n",
    "\n",
    "# Compute the gradients\n",
    "y.backward()\n",
    "\n",
    "# Access the gradients\n",
    "gradients = x.grad\n",
    "\n",
    "# Now print the gradient to the console. It's value should be 4 as we have calculated it before.\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb4a71-be79-454c-83e1-250ad7a5d324",
   "metadata": {},
   "source": [
    "This seems like magic. How does PyTorch know that a tensor was run through a function $f(x) = x^2$. This works because in Python as well as in C/C++, which is the language the core of PyTorch is written in (because of performance optimization), it is possible to overwrite standard operators like plus, minus, and so on. Every time an operator or a function is applied to a tensor PyTorch keep track of this operation. If we then want to compute the gradient of an operation PyTorch simply looks up the path of operations in this graph and uses the chain rule to compute the gradients.\n",
    "\n",
    "With the chain rule it is also possible to compute gradients for multiple chained functions. For example if we define a second function $f_2(x) = ln(x)$, run $x$ first through $f(x)$ and then run the results through $f_2(x)$.\n",
    "\n",
    "First we will run $x$ through $f(x)$ and name the result $h$ (\"hidden\").\n",
    "\n",
    "$$ h = f(x) = x^2 $$\n",
    "\n",
    "The derivative of $f$ with respect to $x$ is $\\frac{\\partial f}{\\partial x} = 2x$ as we already know.\n",
    "\n",
    "Next we run $h$ through $f_2$.\n",
    "\n",
    "$$ f_2(h) = ln(h) $$\n",
    "\n",
    "The derivative of $f_2$ with respect to $h$ is $\\frac{\\partial f_2}{\\partial h} = \\frac{1}{h} = \\frac{1}{x^2}$\n",
    "\n",
    "Now we build a chain of functions defined as $f(x) = f_2(f_1(x))$ which means we first run $x$ through $f_1$ and then run the results through $f_2$. How can we compute the derivative of $f$ with respect to $x$? We can simply do this by multiplying the derivative of $f_2$ with respect to its own input $h$ and the derivative of $f_1$ with respect to its input $x$. This looks like this:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_2}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x} \n",
    "= \\frac{1}{x^2} \\cdot 2x = \\frac{2}{x}\n",
    "$$\n",
    "\n",
    "As $h$ is the same as $f_1(x)$ we could also write it like this:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_2}{\\partial df_1} \\cdot \\frac{\\partial f_1}{\\partial x} $$\n",
    "\n",
    "So for $x = 5$ we should get the following results:\n",
    "\n",
    "$$ \\frac{\\partial f_2}{\\partial h} = \\frac{1}{5^2} = 0.04 $$\n",
    "\n",
    "$$ \\frac{\\partial h}{\\partial x} = 2 \\cdot 5 = 10 $$\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_2}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}\n",
    "= \\frac{2}{x} = \\frac{2}{5} = 0.4 * 10 = 4\n",
    "$$\n",
    "\n",
    "Now let's implement this with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c6d070-665a-48e4-a84e-f8a73b3b4d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h tensor([25.], grad_fn=<PowBackward0>)\n",
      "z tensor([3.2189], grad_fn=<LogBackward0>)\n",
      "Gradient of f2 with respect to f1 0.3999999761581421\n",
      "Gradient of f1 with respect to x: 10.0\n",
      "Gradient of f with respect to x: 3.999999761581421\n"
     ]
    }
   ],
   "source": [
    "# This is our input data\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "\n",
    "# Define the function f2(x) = ln(x)\n",
    "def f2(x):\n",
    "    return torch.log(x)\n",
    "\n",
    "# Apply the functions to the tensor sequentially\n",
    "h = f1(x)\n",
    "z = f2(h)\n",
    "\n",
    "print(\"h\", h)\n",
    "print(\"z\", z)\n",
    "\n",
    "# Compute the gradients of z with respect to h (df2 / df1)\n",
    "z.backward(torch.ones_like(z), retain_graph=True)\n",
    "\n",
    "# Access the gradients with respect to f2 (df2 / df1)\n",
    "gradient_z = x.grad.item()\n",
    "print(\"Gradient of f2 with respect to f1\", gradient_z)\n",
    "\n",
    "# Reset the gradients\n",
    "# Every time we want to compute the gradient with respect to another parameter\n",
    "# we need to delete the old gradients first\n",
    "x.grad.zero_()\n",
    "\n",
    "# Compute the gradient of h with respect to x (df1 / dx)\n",
    "h.backward(torch.ones_like(h), retain_graph=True)\n",
    "\n",
    "# Access the gradients with respect to f1 (df1 / dx)\n",
    "gradient_h = x.grad.item()\n",
    "print(\"Gradient of f1 with respect to x:\", gradient_h)\n",
    "\n",
    "# Use the chain rule to compute the gradient of the chained function\n",
    "gradient_f = gradient_z * gradient_h\n",
    "print(\"Gradient of f with respect to x:\", gradient_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb46cb4-dd45-4b5f-858a-0ad94eacc35c",
   "metadata": {},
   "source": [
    "If we print both $h$ and $z$ to the console we will see the following ouputs:\n",
    "\n",
    "```\n",
    "h tensor([25.], grad_fn=<PowBackward0>)\r\n",
    "z tensor([3.2189], grad_fn=<LogBackward0>\n",
    "```\n",
    "\n",
    "The first value is the value of the tensor when run through the functions f1 and f2. The second value is the derivative function of the operation we ran the tensor through. The first operation was to compute x to the power of two, the corresponding gradient function is named `PowBackward0`. The next operation was to get the logarithm of the result, the corresponding gradient function of the logarithm is named `LogBackward0`.\n",
    "\n",
    "## More about Autograd\n",
    "\n",
    "If you want to read more about PyTorch's autograd functionalityIa highly recommend the following article from the pyTorch documentation:\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "Another very useful site for computing derivatives is WolframAlpha. Check out what you will get when you enter the following term into the search field: `df/dx ln(x^2)`\n",
    "https://www.wolframalpha.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbacbb81-f9cc-4dcc-81fc-2cbdf6b477f1",
   "metadata": {},
   "source": [
    "## Using Autograd to optimize parameters\n",
    "\n",
    "Now that we have seen how autograd works we want to use it for training a simple linear function to fit out data. First we define a linear function and an MSE loss function that compares the predicted y-values to the true y-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a2188d-d890-4c05-b39f-ce764570298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_linear(x: torch.tensor, params: torch.tensor):\n",
    "    \"\"\"\n",
    "    This is a linear function of form y=ax\n",
    "    :param x: Input of the model\n",
    "    :param params: Model parameters\n",
    "    \"\"\"\n",
    "    return params[0] * x + params[1]\n",
    "\n",
    "def mse_loss(y: torch.tensor, y_hat: torch.tensor):\n",
    "    \"\"\"\n",
    "    This is a MSE loss function of form L=(y-y_hat)^2=(y-ax)^2\n",
    "    :param y: True y-values\n",
    "    :param y_hat: Predicted y-values\n",
    "    \"\"\"\n",
    "    return (y - y_hat)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159f191-24ff-4c36-868f-bff9dbe3112f",
   "metadata": {},
   "source": [
    "Next we define data points, the true model parameters and the parameters of the model that are different from the true parameters. We run the x-values through our true function and through our model and then compute the loss using the MSE loss function. Because we want to optimize the parameters later we need to keep track of their gradients. This is why we set the attribute `requires_grad` to `True` for them. X and y values do not change during training, so they are regarded as constants and do need to be optimized, this is why there is no need to compute gradients for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eefab195-436a-41cc-b02d-ff11e257f492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True y-values: tensor([1.0000, 1.5000, 2.0000])\n",
      "Predicted y-values: tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n",
      "MSE Loss: tensor(2.4167, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The input data\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# The model's params need to optimized during training, so gradients should be computed\n",
    "model_params = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "\n",
    "# The true parameters are fixed, so we do not need to compute gradients for them\n",
    "true_params = torch.tensor([0.5, 0.5])\n",
    "\n",
    "# Get true y-values and the predicted y-values of our model\n",
    "y = f_linear(x, true_params)\n",
    "y_hat = f_linear(x, model_params)\n",
    "\n",
    "print(\"True y-values:\", y)\n",
    "print(\"Predicted y-values:\", y_hat)\n",
    "\n",
    "# Compue the loss between the predicted y-values and the true y-values\n",
    "loss = mse_loss(y, y_hat).mean()\n",
    "print(\"MSE Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe688a-24fd-4a23-8135-273a10e713ac",
   "metadata": {},
   "source": [
    "Now we need to compute the gradients by calling the `backward` function on our loss value. Then we can access the gradients of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5abf4b73-8426-4b5c-86fa-3567e777e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for model parameters: tensor([6.6667, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradient of the loss function with respect to x\n",
    "loss.backward()\n",
    "model_params_grad = model_params.grad\n",
    "print(\"Gradients for model parameters:\", model_params_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba901ee-b71c-47db-a92f-7de2176ef94f",
   "metadata": {},
   "source": [
    "We can now use the computed gradients to perform the gradient descent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd8f59c-a141-4884-8825-f603cbed3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated model parameters: tensor([0.6667, 0.8500], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a learning rate and perform the gradient descent step\n",
    "alpha = 0.05\n",
    "model_params = model_params - alpha * model_params_grad\n",
    "\n",
    "# Print the updated parameters\n",
    "print(\"Updated model parameters:\", model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2d6ef-8ff9-449e-b083-1ea7441fbb3a",
   "metadata": {},
   "source": [
    "The updated model parameters are now much closer to the real parameter values. If we repeat this step multiple times we should end up with parameters that are really close to the true parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b01125-b93f-44e6-bfdc-28464361ea80",
   "metadata": {},
   "source": [
    "## Training loop with PyTorch's Autograd Module\n",
    "\n",
    "In this section we build a trainin loop that performs gradient descent multiple times. We repeat the following steps for a number of epochs:\n",
    "\n",
    "- Run data through the model\n",
    "- Compute the loss\n",
    "- Compute the gradients of our parameters\n",
    "- Perform gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079bb888-3916-4424-a50c-7d24c33cbe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=2.4166667461395264, param_a=1.0, param_b=1.0\n",
      "Epoch 1: loss=0.4854629337787628, param_a=0.6666666269302368, param_b=0.8500000238418579\n",
      "Epoch 2: loss=0.10228254646062851, param_a=0.5188888311386108, param_b=0.7816666960716248\n",
      "Epoch 3: loss=0.026139602065086365, param_a=0.4537407159805298, param_b=0.7497222423553467\n",
      "Epoch 4: loss=0.010897613130509853, param_a=0.42538395524024963, param_b=0.734001874923706\n",
      "Epoch 5: loss=0.007738022133708, param_a=0.4134044051170349, param_b=0.72552490234375\n",
      "Epoch 6: loss=0.006978096440434456, param_a=0.4087107181549072, param_b=0.7202915549278259\n",
      "Epoch 7: loss=0.0066972956992685795, param_a=0.40725407004356384, param_b=0.7165202498435974\n",
      "Epoch 8: loss=0.006514647975564003, param_a=0.4072314500808716, param_b=0.7134174108505249\n",
      "Epoch 9: loss=0.006354494486004114, param_a=0.40783995389938354, param_b=0.710629403591156\n",
      "Epoch 10: loss=0.006201764103025198, param_a=0.4087221026420593, param_b=0.7079984545707703\n",
      "Epoch 11: loss=0.006053395569324493, param_a=0.409718781709671, param_b=0.7054541707038879\n",
      "Epoch 12: loss=0.005908723454922438, param_a=0.41075918078422546, param_b=0.7029650211334229\n",
      "Epoch 13: loss=0.005767518188804388, param_a=0.411811888217926, param_b=0.7005167007446289\n",
      "Epoch 14: loss=0.005629710853099823, param_a=0.41286298632621765, param_b=0.6981026530265808\n",
      "Epoch 15: loss=0.005495188757777214, param_a=0.41390639543533325, param_b=0.6957197785377502\n",
      "Epoch 16: loss=0.005363874137401581, param_a=0.4149394631385803, param_b=0.693366527557373\n",
      "Epoch 17: loss=0.005235712509602308, param_a=0.41596105694770813, param_b=0.6910420060157776\n",
      "Epoch 18: loss=0.005110610276460648, param_a=0.416970819234848, param_b=0.6887456178665161\n",
      "Epoch 19: loss=0.004988488741219044, param_a=0.41796866059303284, param_b=0.6864768862724304\n",
      "Epoch 20: loss=0.004869289230555296, param_a=0.4189545512199402, param_b=0.6842354536056519\n",
      "Epoch 21: loss=0.004752941429615021, param_a=0.4199286699295044, param_b=0.6820210218429565\n",
      "Epoch 22: loss=0.004639366175979376, param_a=0.42089107632637024, param_b=0.679833173751831\n",
      "Epoch 23: loss=0.004528518300503492, param_a=0.42184194922447205, param_b=0.6776716709136963\n",
      "Epoch 24: loss=0.0044203042052686214, param_a=0.422781378030777, param_b=0.6755360960960388\n",
      "Epoch 25: loss=0.004314683843404055, param_a=0.423709511756897, param_b=0.6734262108802795\n",
      "Epoch 26: loss=0.004211578983813524, param_a=0.42462649941444397, param_b=0.6713416576385498\n",
      "Epoch 27: loss=0.00411094818264246, param_a=0.4255324602127075, param_b=0.6692821979522705\n",
      "Epoch 28: loss=0.004012716468423605, param_a=0.42642754316329956, param_b=0.667247474193573\n",
      "Epoch 29: loss=0.003916832152754068, param_a=0.42731186747550964, param_b=0.6652372479438782\n",
      "Epoch 30: loss=0.00382324680685997, param_a=0.4281855523586273, param_b=0.6632511615753174\n",
      "Epoch 31: loss=0.003731886623427272, param_a=0.42904871702194214, param_b=0.6612889170646667\n",
      "Epoch 32: loss=0.0036427120212465525, param_a=0.4299015402793884, param_b=0.6593502759933472\n",
      "Epoch 33: loss=0.003555677132681012, param_a=0.43074411153793335, param_b=0.6574349403381348\n",
      "Epoch 34: loss=0.003470703260973096, param_a=0.43157655000686646, param_b=0.6555426120758057\n",
      "Epoch 35: loss=0.00338777550496161, param_a=0.4323989748954773, param_b=0.6536730527877808\n",
      "Epoch 36: loss=0.003306831931695342, param_a=0.4332115054130554, param_b=0.6518259644508362\n",
      "Epoch 37: loss=0.0032278134021908045, param_a=0.43401429057121277, param_b=0.650001049041748\n",
      "Epoch 38: loss=0.003150680335238576, param_a=0.4348074197769165, param_b=0.6481980681419373\n",
      "Epoch 39: loss=0.003075394779443741, param_a=0.4355910122394562, param_b=0.6464167833328247\n",
      "Epoch 40: loss=0.003001905046403408, param_a=0.43636518716812134, param_b=0.6446568965911865\n",
      "Epoch 41: loss=0.002930181100964546, param_a=0.43713006377220154, param_b=0.6429181694984436\n",
      "Epoch 42: loss=0.0028601621743291616, param_a=0.43788573145866394, param_b=0.6412003636360168\n",
      "Epoch 43: loss=0.0027918170671910048, param_a=0.4386323094367981, param_b=0.6395031809806824\n",
      "Epoch 44: loss=0.0027251129504293203, param_a=0.43936991691589355, param_b=0.6378263831138611\n",
      "Epoch 45: loss=0.002659996272996068, param_a=0.44009867310523987, param_b=0.6361697316169739\n",
      "Epoch 46: loss=0.0025964349042624235, param_a=0.4408186674118042, param_b=0.6345330476760864\n",
      "Epoch 47: loss=0.002534392988309264, param_a=0.4415300190448761, param_b=0.6329160332679749\n",
      "Epoch 48: loss=0.0024738346692174673, param_a=0.44223278760910034, param_b=0.6313184499740601\n",
      "Epoch 49: loss=0.0024147231597453356, param_a=0.44292715191841125, param_b=0.6297400593757629\n",
      "\n",
      "Model parameters after training: tensor([0.4436, 0.6282], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# These are the initial model parameters\n",
    "model_params = torch.tensor([1.0, 1.0])\n",
    "\n",
    "# This is our input data\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# These are the true y-values\n",
    "y = torch.tensor([1.0, 1.5, 2.0])\n",
    "\n",
    "# This is the learning rate for our training loop\n",
    "alpha = 0.05\n",
    "\n",
    "for epoch in range(50):\n",
    "    # First we need to create new instances of our data and model parameters because otherwise\n",
    "    # PyTorch get's confused when computing gradients.\n",
    "    x = x.clone()\n",
    "    model_params = model_params.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Run x through the model\n",
    "    y_hat = f_linear(x, model_params)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = mse_loss(y, y_hat).mean()\n",
    "\n",
    "    # Print some information about the learning progress\n",
    "    print(f\"Epoch {epoch}: loss={loss.item()}, param_a={model_params[0]}, param_b={model_params[1]}\")\n",
    "\n",
    "    # Compute gradients (backpropagation step)\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform gradient descent\n",
    "    model_params = model_params - alpha * model_params.grad\n",
    "\n",
    "print(\"\\nModel parameters after training:\", model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd149fbf-41e7-4d42-869f-32564517586d",
   "metadata": {},
   "source": [
    "Before the model was trained the initial values for the parameters were a=1 and b=1. After the training the parameters have become much closer to the true parameter values a=0.5 and b=0.5.\n",
    "\n",
    "The big advantage of PyTorch's autograd module is that we do not implement the definitions of the derivatives ourself. It is enough to define the model and the loss function, the rest is done by PyTorch automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d1613-c68d-470e-9e01-d6dffa0db9de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
